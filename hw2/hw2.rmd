---
title: "StatR 502 Homework 2"
author: "Rebecca Hadi"
date: "Due Thursday, Jan. 18, 2018 at 6:30 pm"
output:
  pdf_document:
    toc: yes
---

Submission guidelines: please submit either a PDF or Word document created in `knitr`. As always, ask in the discussion forum if you have trouble!

## 1: Exploratory modeling with ggplot

Find a dataset of your choice that contains several variables, with at least one categorical variable. _Clearly state_ a hypothesis that you would like to test, based on what your intuition tells you about the data (e.g., I expect that income will be positively correlated with education level and height, and perhaps a weak negative correlation with weight; I also expect there to be a significant difference in incomes between males and females). Construct a linear model that tests your hypothesis, and report the formula for the model with estimated coeffcients and the model fit. Finally, construct a ggplot that illustrates _at least_ four dimensions of the data and the results of your linear model.


First I will load the data and add a feature for region of the US, since state is a granular category variable. 

```{r,include = FALSE}
#Load data 
census <- read.csv("acs2015_census_tract_data.csv")

#Construct Categorical Variable based on state 

NE <- c("Connecticut","Maine","Massachusetts","New Hampshire",
             "Rhode Island","Vermont","New Jersey","New York",
             "Pennsylvania")

MW <- c("Indiana","Illinois","Michigan","Ohio","Wisconsin",
             "Iowa","Kansas","Minnesota","Missouri","Nebraska",
             "North Dakota","South Dakota")

S <- c("Delaware","District of Columbia","Florida","Georgia",
            "Maryland","North Carolina","South Carolina","Virginia",
            "West Virginia","Alabama","Kentucky","Mississippi",
            "Tennessee","Arkansas","Louisiana","Oklahoma","Texas","Puerto Rico")


W <- c("Arizona","Colorado","Idaho","New Mexico","Montana",
            "Utah","Nevada","Wyoming","Alaska","California",
            "Hawaii","Oregon","Washington")


region_list <- list(
  Northeast=NE,
  Midwest=MW,
  South=S,
  West=W)
 
#Add region varaible to census dataset

census$region <- sapply(census$State, 
                 function(x) names(region_list)[grep(x,region_list)])

#add feature for percent male 

census$percent_male <- census$Men / census$TotalPop

```


Hypothesis:  

1. I believe that incomes on the Northeast will be the highest among all regions.



```{r, message=FALSE, warning=FALSE}

#look at data for region 
ggplot(census, aes(x = region, y = Income)) +
      geom_boxplot()
     
    
#Fit Model with log transforms
census_mod <- lm(log(Income) ~  log(White + 1) + log(Men) + factor(region), data = census)
summary(census_mod)      
plot(census_mod,1)


```

Model to test hypothesis: 

$\log(y)$ = 8.544 + 0.21 $\log(White + 1)$ + 0.19 $\log(Men)$ + 0.21Northwest + .001South + .21West

Interpretation of Model: 

* *Intercept:* The predicted log income for a county that is 1% white, has 1 male, and is in the midwest is 8.54, or $5,114.  This is not the most meaningful intercept as it is unlikely for an entire county to have only 1 male. 

* *Coefficient of log(White + 1):* For every 1% increase in the percent white, the predicted income is expected to increase 0.21%. 

* *Coefficient of log(Men):* For every 1% increase in the male population, the predicted income is expected to increase 0.19%.

* *Coefficient of factor(region)Northeast:* Moving from the base case (Midwest) to Northeast is predicted to increase log income by 0.21. 

* *Coefficient of factor(region)South:* Moving from the base case (Midwest) to South is predicted to increase log income by 0.001. This factor is not statistically significant and we can interpret the south region to be similar to midwest region with all other factors equivalent.

* *Coefficient of factor(region)West:* Moving from the base case (Midwest) to West is predicted to increase log income by 0.21. 




**Plotting the model**

```{r, message=FALSE, warning=FALSE}

ggplot(census, aes(x = White, y = Income, col = region, size = Men)) + 
      geom_point(alpha = .2) + 
      theme_classic() + 
      ggtitle ("US Income by % White, Region, and Male Population") 

#Plot model 
plot(census_mod,1)
```



*Hypothesis*:  Given that the factor of Northeast is positive, it would imply that the Northeast region is associated with higher incomes than the Midwest and South regions. However, the coffeficient for West was nearly equivalent to Northeast, which would imply that these regions have similar income.




## Problems from Gelman & Hill

Section 4.9 (pp. 74-75), **do problems 1, 4, 5, 6, 8**. In your write-up, please label them as G&H X, where X is the problem number.

When G&H refer to a "folder" for data, it can be found and downloaded online at this site: <http://stat.columbia.edu/~gelman/arm//examples/>.


## G&H 1

1. Logarithmic transformation and regression: consider the following regression:

$log(weight)$ = ???3.5 + 2.0 $log(height)$ + error

with errors that have standard deviation 0.25. Weights are in pounds and heights
are in inches.

(a) Fill in the blanks: approximately 68% of the persons will have weights within
a factor of (blank) and (blank) of their predicted values from the regression.

```{r,message=FALSE, warning=FALSE}
#0.25 is the standard deviation for log(weight).
#To find the range for weight, we need to exponentiate 0.25 

lower_bound <- exp(-.25)
upper_bound <- exp(.25)

```

**68% confidence interval for weight:**

Lower Bound is ```r lower_bound``` 
Upper Bound is ```r upper_bound``` 


(b) Draw the regression line and scatterplot of log(weight) versus log(height) that
make sense and are consistent with the fitted model. Be sure to label the axes
of your graph.

To create a scatterplot, I will need to create vectors of data for heights and weights, then take the log of those vectors. It is reasonable for men and women to have a different average height, so I will simulate data with slightly different means.

[Source for average female and male height] (https://en.wikipedia.org/wiki/List_of_average_human_height_worldwide)
*Note: Standard deviations are assumed* 


```{r, message=FALSE, warning=FALSE}

#calculate 50 observations for female and male
female_heights <- rnorm(50, mean = 63.5, sd = 3)
male_heights <-  rnorm(50, mean = 69, sd = 3)

#Combined into single vector 
heights <- c(female_heights,male_heights)

#log transform
log_heights <- log(heights)


#Predict weights based on formla given in problem (add in error term)

log_weights <- -3.5 + 2 * log_heights + rnorm(length(log_heights), mean = 0, sd = 0.25)


#Combine log_weights and log_heights into one data frame 

df <- data.frame(cbind(log_weights, log_heights))
colnames(df) <- c("log_weights", "log_heights")

#Run linear model based on log_weights and log_heights 
mod1 <- lm(log_weights ~ log_heights)


#create plot of data 

plot <- ggplot(df, aes(x = log_heights, y = log_weights)) + 
        geom_point() + 
        labs(x = "log (heights)", 
             y = "log (weights)") + 
        theme_classic() +
        ggtitle("Log Heights vs. Log Weights") + 
        geom_smooth(method = "lm", formula = y ~ x)


plot

```





## G&H 4

Logarithmic transformations: the folder pollution contains mortality rates and
various environmental factors from 60 U.S. metropolitan areas (see McDonald
and Schwing, 1973). For this exercise we shall model mortality rate given nitric
oxides, sulfur dioxide, and hydrocarbons as inputs. This model is an extreme
oversimplification as it combines all sources of mortality and does not adjust for
crucial factors such as age and smoking. We use it to illustrate log transformations
in regression.

(a) Create a scatterplot of mortality rate versus level of nitric oxides. Do you
think linear regression will fit these data well? Fit the regression and evaluate
a residual plot from the regression.

```{r,message=FALSE, warning=FALSE}
#Load data
pollution <- read.dta("pollution.dta")

#Plot data 

pollution_plot <- ggplot(pollution, aes(x = nox, y = mort)) + 
                  geom_point() + 
                  labs(x = "Nitric Oxide", 
                       y = "Mortality Rate") + 
                  theme_classic() + 
                  ggtitle("Nitric Oxide vs. Mortality Rate")

pollution_plot


```


**Fitting the model** 

```{r, warning=FALSE, message=FALSE}

#fitting and displaying model
mod2 <- lm(mort ~ nox, data = pollution)
summary(mod2)


#Display residual plot 
plot(mod2,1)

```


Since many of the data points are concentrated between 0 and 50 with a high vertical scale, it seems like a logarithmic transformation may help with fit.  Additionally, the dynamic range is quite wide, which implies that a logistic transformation may help. 

The errors in the residual plot do not follow an even/random distribution. 



(b) Find an appropriate transformation that will result in data more appropriate
for linear regression. Fit a regression to the transformed data and evaluate
the new residual plot.

**Applying Logistic Regression** 
 
```{r, message= FALSE, warning = FALSE}

log_mort <- log(pollution$mort)
log_nox <-  log(pollution$nox)


mod3 <- lm(log_mort ~ log_nox)
summary(mod3)


#Plot residuals 
plot(mod3,1)
```


The residual plot appears much more normal after applying the log transformation, although there appears to be some underlying pattern which may suggest we are missing predictors. 


(c) Interpret the slope coefficient from the model you chose in (b).

* *Coefficient of log_nox*:  For each 1% change in Nitric Oxide pollution potential, there is a 0.02% increase in total age-adjusted mortality rate per 1,000. 




(d) Now fit a model predicting mortality rate using levels of nitric oxides, sulfur
dioxide, and hydrocarbons as inputs. Use appropriate transformations when
helpful. Plot the fitted regression model and interpret the coefficients.

First, I want to inspect the relationship between variables to see if a log transform is appropriate. 

```{r, message=FALSE, warning=FALSE}

#Plot HC vs. Mort to see what type of transform may be appropriate
ggplot(pollution, aes(x = hc, y = mort)) + 
      geom_point() + 
      theme_classic() + 
      ggtitle("Hydrocarbon vs. Mortality Rate")

#Plot sulfur vs. Mort to see what type of transform may be appropriate
ggplot(pollution, aes(x = so2, y = mort)) + 
      geom_point() + 
      theme_classic() + 
      ggtitle("Sulphur Dioxide vs. Mortality Rate")


```

Based on these outputs, it appears that Hydrocarbon could use a transform. I'm less convinced that sulphur needs a transform, but it will help with model interpretability since I am transforming other metrics, so I will apply one as well. 

```{r, message=FALSE, warning=FALSE}

#Transform variables
log_hc <- log(pollution$hc)
log_so2 <-  log(pollution$so2)


#Fit model
mod4 <- lm(log_mort ~ log_nox + log_hc + log_so2)
summary(mod4)


#Plot fitted model 
plot(mod4)

```

**Interpreting the model** 

* *Intercept:*  The intercept represents the log mortality when log_nox, log_hc, and log_so2 are equal to 0. Taking the exponent to undo the log transform, when Nitric oxide, Hydrocarbon, and Sufphur Dioxide are all equal to 1 ($\log(0)$ = 1), then the predicted value of mortality rate per 1,000 is 922.1427 (from $exp(6.826749)$).

* *Coefficient of log_nox:*   For every 1% change in Nitric Oxide, there is a .06% change in predicted mortality rate per 1,000.  This coefficient is statistically significant. 

* *Coefficient of log_hc:* For every 1% change in Hydrocarbon, there is a -0.06% decrease in mortality rate per 1,000. This coefficient is statistically significant.    

* *Coefficient of log_so2:* For every 1% change in Sulphor Dioxide, there is a .014% increase in mortality rate per 1,000. This coefficient is *not* statisically significant, but is logical and doesn't make a huge impact on the model to include. 




(e) Cross-validate: fit the model you chose above to the first half of the data and
then predict for the second half. (You used all the data to construct the model
in (d), so this is not really cross-validation, but it gives a sense of how the
steps of cross-validation can be implemented.)

```{r, message=FALSE, warning=FALSE}
#Split dataset (first 30 rows into train, latter 30 into test)
train <-pollution[1:30, ]
test <- pollution[31:60, ]



#fit model on training data set
mod5 <- lm(log(mort) ~ log(nox) + log(hc) + log(so2), data = train)
summary(mod5)


#Predict on the test data set
predictions <- predict(mod5, test)

#Compare predictions to actuals 
compare <- cbind(exp(predictions),test)

comparison <-(exp(predictions) - test$mort)

#find average and sd error
mean(comparison)
sd(comparison)

```




## G&H 5
Special-purpose transformations: for a study of congressional elections, you would
like a measure of the relative amount of money raised by each of the two major party
candidates in each district. Suppose that you know the amount of money
raised by each candidate; label these dollar values $D_i$ and $R_i$. You would like to
combine these into a single variable that can be included as an input variable
into a model predicting vote share for the Democrats.

(a) Discuss the advantages and disadvantages of the following measures:
* The simple difference, $D_i$ ??? $R_i$
*   *Advantages:* An advantage of this transformation is that it is centered at zero. 
*   *Disadvantages:* Possible for values to be negative so we could not use a log transform on this variable if desired.  Also does not take scale into account, e.g. if $D_i$ = $10M and $R_i$ = $9M then $D_i$ - $R_i$ = $1M, which is the same value as if  $D_i$ = $1M and $R_i$ = $0M, which we would expect the latter to make a large difference in vote share than the former, which the model would likely not capture. 


* The ratio, $D_i$/$R_i$
*   *Advantages:* It scales the contribution, which was the problem with the simple difference transform.    
*   *Disadvantages:* When Democrats raise more money than Democrats, the ratio can go to infinity, but when the Republicans raise significantly more than the Democrats, the ratio just tends more and more toward zero. The implication to the model is that it will likely place a higher weight on the case where Democrats raise more than Republicans. 


* The difference on the logarithmic scale, $log(D_i)$ ??? $log(R_i)$
*   *Advantages:*  Scales the contributions so that the same delta in a lower raising area will have more weight. It is less sensitive to outliers than the $D_i$ / $R_i$ transformation.      
*   *Disadvantages:*  Coefficients may need transformation to be interpretable.   


* The relative proportion, $D_i$/($D_i$ + $R_i$).
*   *Advantages:*  Keeps the input variable on a scale between 0 and 1.  
*   *Disadvantages:*  Doesn't give more weight to contributions in a low raising area.  


(b) Propose an idiosyncratic transformation (as in the example on page 65) and
discuss the advantages and disadvantages of using it as a regression input.

One transformation could be to map the contributions into ranges in low, medium, and high for each group. One advantage is that you could understand the impact of "high" vs. "low" contributions on voter share. One disadvantage is that you lose the ability to measure the magnitude of small changes in contribution amount. 


## G&H 6

An economist runs a regression examining the relations between the average price
of cigarettes, P, and the quantity purchased, Q, across a large sample of counties
in the United States, assuming the following functional form, $\log(Q)$ = $\alpha$+ $\beta$ log P.
Suppose the estimate for ?? is 0.3. Interpret this coefficient.

For every 1% difference in P, there is a difference of 0.3% in the prediction for Q. 


## G&H 8

Return to the teaching evaluations data from Exercise 3.5. Fit regression models
predicting evaluations given many of the inputs in the dataset. Consider interactions,
combinations of predictors, and transformations, as appropriate. Consider several models, discuss in detail the final model that you choose, and also explain why you chose it rather than the others you had considered.


*Note: I initially had summary(modX) after each model, but removed as they were not needed in the final output* 
```{r, message = FALSE, warning=FALSE}
beauty <- read.csv("ProfEvaltnsBeautyPublic.csv")

#see distribution of average beauty score
beauty_plot <- ggplot(beauty, aes(btystdave)) + 
               geom_histogram(bins = 20)


#see distribution of age
age_plot <- ggplot(beauty, aes(age)) + 
               geom_histogram(bins = 20)



#Simple Model 
mod6 <- lm(courseevaluation ~ btystdave, data = beauty)

#Add age 
mod7 <- lm(courseevaluation ~ btystdave + age, data = beauty)

#Transform age for interpretability (center around mean)
z_age <- (beauty$age - mean(beauty$age)) / sd(beauty$age)

#Fit model with centered age (will not change fit but will make coefficients more interpretable)
mod8 <- lm(courseevaluation ~ btystdave + z_age, data = beauty) 



#Add gender factor (Keep in summary() because this is the chosen model)
mod9 <- lm(courseevaluation ~ btystdave + z_age + factor(female), data = beauty) 
summary(mod9)


#Add interaction between age and gender 
mod10 <- lm(courseevaluation ~ btystdave + z_age * factor(female), data = beauty) 


#compare models to see which has a better fit 
BIC(mod6, mod7, mod8, mod9, mod10)


```


The final model I've chosen can be interpreted as follows:
 
* *Intercept*: The predicted course evaluation for the base case of a male professor with an average beauty rating and who is 50.74 years old (average age for male) is 4.09942.
* *Coefficient of btystddave*: For every 1 pt increase in average beauty score (holding all other variables constant), the predicted course evaluation is expected to increase 0.13998.
* *Coefficient of Z_age*: The predicted difference in course evaluation for a 1 standard deviation difference in age, for a male, is -0.0255. This coefficient is not statistically significant, but is plausible to include in the model and does not dramatically impact prediction. 
* *Coefficient of factor(female)*: The predicted difference in course evaluation for a female, assuming the average beauty score and average age, would decrease -0.21079 for a female compared to a male. 


I chose mod9 because after comparing BIC scores across the various models, mod9 had a BIC difference of roughly 4 between mod6 (my base case with only average beauty score as the predictor), suggesting that there is moderate evidence that this model outperforms the base model.  
